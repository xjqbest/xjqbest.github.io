---    
layout: post
title:  "机器学习编程框架的调度模块"
date:   2018-07-13 11:00:00
categories: MachineLearning
tags: MachineLearning
excerpt: 
---


# 1 机器学习框架的三种角色

一个机器学习框架一般是有三种角色：PServer、Worker和Master(或叫Scheduler)。

PServer即Parameter Server（参数服务器），作用是保存模型训练过程中的所有参数。
即是分布式存储key-value对，每个节点上保存一部分key-value对。

Worker模块属于执行者的角色，即执行具体的计算逻辑。

Master是机器学习作业中作业的管理者，管理ParameterServer、Worker的存活性，管理作业的资源申请和释放以及作业的进度等。

本文讲一讲Master模块。

# 2 Master角色的具体职责

## 2.1 申请释放资源

### 2.1.3 申请资源

所有pserver和worker节点的资源，由master节点负责向资源调度系统申请和释放。

一个机器学习作业首先启动的节点是master节点，然后由master节点根据用户的配置，向资源调度系统申请pserver的资源。

这里之所以首先申请pserver的资源而不是worker，因为一般pserver要先于worker启动和准备接收来自worker的pull/push请求。

当然如果资源不够，master会不断轮询直到分到足够数量的pserver。当pserver启动并正常运行时，接着再申请worker资源。

#### 2.1.3.1 动态扩缩容

动态扩缩容指worker/pserver节点数目允许动态改变，而整个作业几乎不受影响。

一般来说对于worker，允许只启动一部分，整个作业即可跑起来。对于pserver需要全部启动起来，然后再继续接下来的流程，比如申请worker资源。

##### 对于worker节点
一般来说，worker节点数目的动态改变相对容易，增加节点即是新增的节点分到一批数据然后开始计算即可。

而减少节点一般是由于资源抢占或者某些节点上作业的进程异常退出，由于master上维护了每个节点已完成计算和未完成计算的数据，
因此只需要把这些节点上未完成计算的数据分给其他节点即可。  

##### 对于pserver节点

对于pserver节点的数目动态改变则相对困难

因为每个pserver节点上保存一部分参数，我们不能因为某一pserver节点失败而完全丢弃该参数，
因此需要备份，还需要一个一致性算法来保证每个副本的一致性。

通常每个参数根据key哈希到某一pserver上，hash得到的结果范围是固定的。
如果pserver数目增加，那么需要所有key重新计算哈希的结果，分到新的pserver上，开销太大。
不过可以用一致性哈希算法来解决该问题。

### 2.1.4 释放资源

释放资源通常有两种情况，一种是作业中途失败时，另一种是作业成功执行完后。

## 2.2 数据分片

通常输入数据是一个目录，底下有很多文件。

master根据配置，将所有数据切分成分发给worker的最小单位，本文称为part。比如我们可以直接将每个文件作为一个part。

分发时，假如我们设置了每次最多分发给worker的part数目为5，那么每次就分给worker 5个文件，等它算完了，再继续分配。

如果是采用异步更新，那么速度快的worker可以处理更多的part，一定程度上缓解了分布式计算中常见的“慢节点问题”。

## 2.3 心跳控制

我们把分布式系统中slave节点与master节点之间周期的通信称为心跳。

正如它的名字，心跳的作用是master确定每个slave节点（worker/pserver）还活着，并可以在master回复给slave节点的消息中附带一些控制信息。

心跳可以有很多类型，比如：

- register心跳：worker刚启动时，第一个发给master的心跳类型为register类型。

- Normal心跳：一般不带控制信息，只表示该节点还活着。

- Assign心跳：worker处理完数据后，master继续分发一批数据给worker。

- Exit心跳：worker／pserver节点的退出心跳。

上面之所以加了register心跳，主要考虑是允许一部分节点注册后作业就running起来 还是 必须等所有节点注册后作业再running起来。

## 2.3.1 心跳超时

心跳超时是指一段时间内，worker/pserver节点没有向master发送心跳的现象。
这通常意味着该节点可能失败，或者由于网络拥堵等原因。

一旦心跳超时，master就简单的认为该节点超时，并触发容错（failover）。

## 2.4 状态和状态转移

master节点内部维护了当前作业的状态，并根据特定的动作进行相应的状态转移。

master通常将作业的状态大致划分为以下几个阶段：  
1. new：最开始的状态，什么也没做

2. initing：master切分数据中

3. init：切分数据完成，并做一些初始化工作

4. submitted：向资源调度系统申请资源中

5. submitted：向资源调度系统申请资源返回，由于一般是异步接口，还需要轮询查看分没分到资源以及分了多少资源

6. scheduled：分配到了资源

7. running：部分worker启动，所有pserver启动

8. fail或者exit：失败或者成功退出

master内部还维护了每个worker分到的数据，和已经处理完的数据。

## 2.5 监控worker/pserver进程状态

master通过资源调度系统监控worker/pserver进程状态，主要是用在master确认这些进程退出后，自己再退出。

还可以用在作业结束后上传日志，虽然作业结束，但是进程还是由于在上传日志而没有退出，
master则等待日志上传完毕而进程退出 或者 超过一定时间强行让其退出。

# 3 Master的可优化之处

## 3.1 容错

Master在运行中的主要功能是维护状态和状态的转移，因此可以采用的方式是：

1. 单个master，定期把维护的状态保存起来，比如保存到本地或hdfs（checkpoint）。

2. 单个master，直接将状态保存在某一自带容错和一致性的系统中，比如etcd。

3. 多个master，其中一个主master，其他副本master保存的状态一致。这样其中一个挂了，其他的还可以用。

## 3.2 预测执行

类似于spark中的speculative execution，对于一个拖后腿的task，会在其他节点上再次执行这个task，如果其中一个task跑成功则将这个最先完成的task的计算结果作为最终结果，同时会干掉其他节点上运行的该task。

在本文中，这个task可以是一个worker节点 或者 一批数据。

## 3.3 同步

上文所讲的都是数据并行下的纯异步模式。不能额满足所有算法的需求，并且同步模式一般来说收敛更快。

在pserver框架中通常采用mini-batch的方式来更新梯度，也即每一轮中，每次算一个min-batch后更新一次梯度。

min-batch通常是设置的一批数据的大小。
我们首先把数据中的每k行作为一个数据块，称为block，那么可以设置每个min-batch包含m个block。

因此如果我们在每算完min-batch后做一次同步，对于分布式worker来说，就是所有worker一共算完m个block后做一次同步。

由master节点计算每个worker需要计算的block数目，等所有worker算完之后，再开始下一次同步。

## 3.4 worker数据保留在内存中

如果每个worker每一轮分到同样的一批数据，就没必要每次重新下载再读到内存中，可以一直保留在内存中直到作业结束。

当然要注意的就是机器的内存要足够大。

## 3.5 单机模式

上文说的都是分布式的跑作业，如果想在本地自己写一些demo跑很小的数据集，那么就需要做一些改动，比如master申请释放资源。

申请释放资源在本地就是通过fork子进程作为pserver/worker，作业结束后再kill这些子进程来实现的。

## 3.6 机器黑名单

如果在某一机器上的节点失败超过一定次数，或者运行在该机器上的节点大多数是慢节点，则将该机器加入黑名单，尽量不在这些机器上申请资源。过一段时间后再放出黑名单。进入黑名单一定次数后，便不再从黑名单中移除。

