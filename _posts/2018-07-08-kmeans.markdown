---    
layout: post
title:  "常见聚类算法"
date:   2018-07-08 11:00:00
categories: MachineLearning
tags: MachineLearning
excerpt: 
---

# 1 K-Means

## 1.1 算法流程：

输入：N个数据点， 类簇个数K

输出：K个类簇的均值向量

流程：

1. 随机选择K个样本点作为初始的均值向量(即中心点)

2. 不断循环以下步骤，直到收敛（达到最大迭代轮数或者均值向量的调整幅度小于阈值）

3. 对每个样本点i，计算离它最近的均值向量$$ C_k $$，并将样本点i归为第k类

4. 计算每个类簇新的中心点（取均值）


## 1.2 聚类的评价指标(不仅限于KMeans)

无监督聚类算法的评价指标大致可以分为两大类：一类是，聚类的结果具有某个参考模型作为人为基准进行比较，称之为外部指标；第二种是：直接考察聚类结果而不参考任何模型，称之为内部指标。

### 1.2.1 外部指标

假定通过聚类算法将样本聚为一个类簇的集合$$ C $$，参考的已有模型将样本聚为一个类簇的集合$$ C^{\star} $$。
对于每两个样本

\begin{align}
a&=\vert S_1 \vert, 其中 S_1=\\{(x_i,x_j) \vert \lambda_i = \lambda_j,\lambda_{i}^{\star} = \vert \lambda_j^{\star},i<j\\} \\\
b&=\vert S_2 \vert, 其中 S_2=\\{(x_i,x_j) \vert \lambda_i = \lambda_j,\lambda_{i}^{\star} != \vert \lambda_j^{\star},i<j\\} \\\
c&=\vert S_3 \vert, 其中 S_3=\\{(x_i,x_j) \vert \lambda_i != \lambda_j,\lambda_{i}^{\star} = \vert \lambda_j^{\star},i<j\\} \\\
d&=\vert S_4 \vert, 其中 S_4=\\{(x_i,x_j) \vert \lambda_i != \lambda_j,\lambda_{i}^{\star} != \vert \lambda_j^{\star},i<j\\} 
\end{align}

其中集合$$ S_1 $$包含了在$$ C $$中属于相同的簇并且在$$ C^{\star} $$中也属于相同的簇的样本  
$$ S_2 $$包含了在$$ C $$中属于相同的簇并且在$$ C^{\star} $$中不属于相同的簇的样本  
$$ S_3 $$包含了在$$ C $$中不属于相同的簇并且在$$ C^{\star} $$中属于相同的簇的样本  
$$ S_4 $$包含了在$$ C $$中不属于相同的簇并且在$$ C^{\star} $$中不属于相同的簇的样本  

基于jaccard的评价指标为$$ JCI = \frac{a}{a+b+c} $$，该值在[0,1]之间，并且值越大越好。


### 1.2.2 内部指标

每两个类簇之间，类簇内的节点间平均距离 除以 这两个类簇中心点的距离。

\begin{align}
 DBI=\frac{1}{k}\sum_{i=1}^k\max_{j\ne i}\bigl(\frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu_i,\mu_j)}\bigl) 
\end{align}

其中$$ \mu $$表示中心点。上面这个式子的值越小越好。

## 1.3 如何确定K的值呢

如果设定的K值小于真实簇数量，随着K的增加，指标会迅速提升

而如果设定的K值大于真实簇数量，随着K的增加，指标提升会变缓

由此可以找到一个较好的K值


## 1.4 初始化中心点

除了随机初始化外，还可以采用Canopy方法：

设置两个阈值$$ T_1 > T_2 $$

流程：

（1）将所有数据点作为一个集合

（2）迭代至集合为空  
1. 从集合中随机选取一个点作为簇的中心
2. 对剩余的每个数据点，如果其与某个簇的距离小于$$ T_1 $$，归到该簇中
3. 如果其与该簇的距离小于$$ T_2 $$，从集合中删除这个点


可以看出Canopy优点是不需要实现确定K值，缺点是需要设置$$ T_1 $$和$$ T_2 $$，
$$ T_2 $$的值如果过小经常会导致簇的个数过多。

## 1.5 分布式实现

1. 首先由某一机器（比如0号机器）随机选取K个中心点，然后广播给其他机器

2. 计算每个节点与中心点的距离，并归入最小距离对应的中心点的类簇中

3. 计算每个类簇的新的中心点（求均值向量）

个人认为对第3步来说，如果节点数目太多，向量每个维度求和，最后才除以个数求平均的话，可能溢出了，
所以可以每个机器分别求均值，然后所有机器再求一次均值。

# 2 DBSCAN

DBSCAN全称Density-based spatial clustering of applications with noise，即带噪声的基于密度的空间聚类。

## 2.1 几个概念

$$ \epsilon $$邻域：一个点的半径$$ \epsilon $$内的区域称为该点的$$ \epsilon $$邻域

核心点（core point）： 如果给定对象$$ \epsilon $$邻域内的样本点数大于等于MinPts，则称该对象为核心对象

直接密度可达（directly density-reachable）：
给定一个点的集合D，如果p在q的$$ \epsilon $$的邻域内，且q是一个核心点，则我们说p点直接可达q点。

密度可达（density-reachable）：
如果存在一个序列$$ p_1,p_2,...,p_n $$，其中$$ p_1,p_2,...,p_{n−1} $$ 都是core point，
且$$ p_{i+1} $$都是$$ p_i $$的直接可达，则称$$ p_n $$可达$$ p_1 $$。可达不是对称的。

密度相连（density-connected）：
已知有点p，q，如果另存在点o且o既密度可达p也密度可达q，则点p和q密度相连。相连是对称的。

簇（Cluster）：所有密度相连的点构成的集合为一个簇。

## 2.2 简单的分布式实现

1. 每台机器分到一部分数据，本地两两计算距离，小于$$ \epsilon $$则视为一条边连接，构建无向图的邻接表
2. 根据邻接表标记core point
3. 将每个core point与其邻接点构建集合
4. 集合两两尝试合并（若有相同的core point则合并）
5. 当任意两个集合都没有相同的core point结束，此时剩余的每个集合都是一个簇

 个人想法是在第4步，通过reduce到0号机器，拿到所有的core point，然后求core point之间是否两两直接可达。

 现在假设求出了每个core point对应的直接可达的集合，那么集合两两合并（若有相同的core point则合并）。

 合并之后，再广播给所有节点最终的类簇和每个类簇对应的所有core point。然后再由每台机器各种输出每个样本点对应的类簇即可。

## 2.3 优化

其实可以发现DBSCAN中很关键的一个部分是决定数据集中每个点是否是核心点。
传统DBSCAN做法是计算每个点之间的距离，并以此判断各点的邻近点数量，
算法复杂度是$$O（n^2)$$。事实上，决定一个点是否为核心点，只需要参考这个点与“周围”的点的距离即可，
并不需要计算这个点和其余所有点之间的距离。

有一个思路是将数据集划成一个个的小格，每个小格就是一个“周围”。

比如对于二维的情况，将数据集的空间划分成各个以$$ \epsilon / \sqrt 2 $$为边长的正方形小格（称为grid），
则每个点唯一对应的grid也可以确定下来，具体为各点坐标除以grid的边长，取下界整数。

### 2.3.1 好处

这么定义grid有如下好处：

1. 首先，每个小格的对角线长度为$$ \epsilon $$，是每个小格内的最长距离。
如果在同一个小格中的点数量超过了参数minPts，那么不需要计算这些点的邻近点数量，就可以直接判断小格子里的所有点都为核心点

2. 对那些包含点数量没超过minPts的小格，只需要去邻近的小格中找点即可。


# 参考资料

[https://www.cnblogs.com/549294286/p/3622097.html](https://www.cnblogs.com/549294286/p/3622097.html)

[http://willzhang4a58.github.io/2016/06/kmeans/](http://willzhang4a58.github.io/2016/06/kmeans/)

[https://blog.csdn.net/cokeonly/article/details/70173567](https://blog.csdn.net/cokeonly/article/details/70173567)