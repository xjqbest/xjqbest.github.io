---
layout: post
title:  "GPU学习"
date:   2020-04-05 10:04:00
categories: GPU
tags: GPU
excerpt: GPU学习
---

### paddle-box论文

[paddlebox-Paper.pdf](/docs/learn_gpu/paddlebox-Paper.pdf)

[balla(02-08-00)-02-09-40-1408-distributed_hie.pdf](/docs/learn_gpu/paddlebox-ppt.pdf)


### 书

[book1.pdf](/docs/learn_gpu/book1.pdf)


### gpu通信

1. [https://www.infoq.cn/article/3D4MsRVS8ZOtGCj7\*krT](https://www.infoq.cn/article/3D4MsRVS8ZOtGCj7*krT)

2. [http://server.it168.com/a2018/0604/3206/000003206891.shtml](http://server.it168.com/a2018/0604/3206/000003206891.shtml)

3. [https://blog.csdn.net/weixin_34186128/article/details/89717186](https://blog.csdn.net/weixin_34186128/article/details/89717186)

4. [https://blog.csdn.net/weixin_33709609/article/details/89687048](https://blog.csdn.net/weixin_33709609/article/details/89687048)



### 一个还不错的gpu入门系列

1. [简要介绍了一下nvidia的发展历史](https://juejin.im/post/5bd1e767f265da0afb3389f8)

2. [cuda](https://juejin.im/post/5bc9f83b5188255c8a06307d)

3. [gpu架构](https://juejin.im/post/5bcdf5715188251a29718b85)

4. [CPU与GPU的矩阵乘法对比](https://juejin.im/post/5c458b036fb9a049ba41e4a8)

5. [并行规约优化](https://juejin.im/post/5c683450518825621d0b9608) 

6. [利用好shared memory](https://juejin.im/post/5c6a31afe51d457fa31e6324)


### 一些基础知识

 - GPU的相比CPU有几个特点：内存带宽大（十倍）、内存延迟高（CPU 使用多级缓存掩盖延迟，GPU 采用多线程掩盖延迟）、寄存器多。GPU 只适合处理分支少，数据量大，延迟不敏感的任务。

     - GPU 不适合处理大量分支：GPU 控制部件面积比较小，为了节约控制器，32 个 CUDA Core 必须时刻执行同样的指令。也就是说，一个 Warp 内部的所有 CUDA Core 的 PC（程序计数器）一直是同步的。但是访存地址是可以不同的，每个核心还可以有自己独立的寄存器组，这种执行方式叫做 SIMT（Single Instruction Multi Trhead）。
     - GPU 需要数据高度对齐。一个一个 Warp 的内存访问是成组的，一次只能读取连续的且对齐的 128byte（这正好是WarpSize 32 \* 4 byte）。

       <img src="/images/learn_gpu/3.png" width="70%" height="70%">
    
     - GPU 访存延迟大: 一个SM最多可同时启动 1024 个线程，但是一个 SM 中仅有 4个 Warp 共计 4 \* 32 = 128 个 CUDA Core。显然一个SM可以启动的线程数比 CUDA Core 的数量大好多。这个 Warp在等数据准备好，我们可以执行另外一组32个线程，这样虽然延迟还是很大，但是 CUDA Core 和 Memory 都能够充分利用。

     - GPU 的线程切换不同于 CPU： 在 CPU 上切换线程需要保存现场，将所有寄存器都存到主存中，而我们最开始说了，一个 SM 中有高达 64k个4 bytes 寄存器。而每个 Thread 最高使用的寄存器数量为255。256 * 4 * 32 = 32k。也就是说我每个线程把寄存器用到爆，也才用了一半的寄存器。GPU 的线程切换只是切换了寄存器组，延迟超级低。线程切换是以 Warp 为单位。

 - cuda编程模型

    <img src="/images/learn_gpu/4.png" width="70%" height="70%">


### cuda stream

[https://www.cnblogs.com/1024incn/tag/CUDA/](https://www.cnblogs.com/1024incn/tag/CUDA/)

[https://devblogs.nvidia.com/how-overlap-data-transfers-cuda-cc/](https://devblogs.nvidia.com/how-overlap-data-transfers-cuda-cc/)

[https://www.cnblogs.com/1024incn/p/5891051.html](https://www.cnblogs.com/1024incn/p/5891051.html)

[https://zhuanlan.zhihu.com/p/51402722](https://zhuanlan.zhihu.com/p/51402722)

[https://www.cnblogs.com/biglucky/p/4313266.html](https://www.cnblogs.com/biglucky/p/4313266.html)

[http://tech.it168.com/a2011/0708/1215/000001215209_1.shtml](http://tech.it168.com/a2011/0708/1215/000001215209_1.shtml)

一个stream是在GPU上执行的一个操作序列，不同的stream之间的操作可以交叉或者同时进行。

有两种stream：

 - default stream(空流)：是一个同步stream。no operation in the default stream will begin until all previously issued operations in any stream on the device have completed, and an operation in the default stream must complete before any other operation (in any stream on the device) will begin

 - non-default stream(非空流)：由cudaStreamCreate创建。


非空流也分阻塞和非阻塞：非空流与空流之间是否会产生阻塞影响


对于host而言，cudaMemcpy是同步的，Kernel的launch是异步的。stream的同步可以用cudaStreamSynchronize。

cudaMemcpyAsync要求Pinned memory，即不可分页内存，保证host上的内存物理地址不变。

CUDA device有两种engine：

 - copy engine，包括H2D engine 和 D2H eigine

 - kernel engine


同步：

 - cudaDeviceSynchronize：host等待所有device上的运算或者数据传输操作完成
 - cudaStreamSynchronize：使host等待特定stream中的操作全部完成或者使用非阻塞版本的cudaStreamQuery来测试是否完成
 - cudaEventSynchronize：可以用来实现更细粒度的同步


### 一些名词

**pin memory**：锁页内存，锁页内存存放的内容在任何情况下都不会被换页到硬盘上

**PCIe**：IO总线

<img src="/images/learn_gpu/2.png" width="70%" height="70%">

**NVSwitch**:为了解决混合立方网格拓扑结构的问题，类似于PCIe使用PCIe Switch用于拓扑的扩展，NVIDIA使用NVSwitch实现了NVLink的全连接。NVSwitch作为首款节点交换架构，可支持单个服务器节点中 16 个全互联的 GPU，并可使全部 8 个 GPU 对分别以 300 GB/s 的惊人速度进行同时通信。这 16 个全互联的 GPU (32G显存V100)还可作为单个大型加速器，拥有 0.5 TB 统一显存空间和 2 PetaFLOPS 计算性能。

**GPUDirect RDMA**，就是计算机1的GPU可以直接访问计算机2的GPU内存。而在没有这项技术之前，GPU需要先将数据从GPU内存搬移到系统内存，然后再利用RDMA传输到计算机2，计算机2的GPU还要做一次数据从系统内存到GPU内存的搬移动作。GPUDirect RDMA技术使得进一步减少了GPU通信的数据复制次数，通信延迟进一步降低。

<img src="/images/learn_gpu/1.png" width="80%" height="80%">

**InfiniBand**:IP网络协议如TCP/IP，具有转发丢失数据包的特性，网络不良时要不断地确认与重发，基于这些协议的通信也会因此变慢，极大地影响了性能。与之相比，IB使用基于信任的、流控制的机制来确保连接的完整性，数据包极少丢失。使用IB协议，除非确认接收缓存具备足够的空间，否则不会传送数据。接受方在数据传输完毕之后，返回信号来标示缓存空间的可用性。通过这种办法，IB协议消除了由于原数据包丢失而带来的重发延迟，从而提升了效率和整体性能。

**warp**:是SM的基本执行单元。
 - 一个warp中的线程必然在同一个block中，如果block所含线程数目不是warp大小的整数倍，那么多出的那些thread所在的warp中，会剩余一些inactive的thread，也就是说，即使凑不够warp整数倍的thread，硬件也会为warp凑足，只不过那些thread是inactive状态，需要注意的是，即使这部分thread是inactive的，也会消耗SM资源，这点是编程时应避免的。
 - Warp Divergence(warp分歧)：为了获得最好的性能，需要避免同一个warp存在不同的执行路径。
 - grid和block的配置准则：保证block中thread数目是32的倍数。避免block太小：每个blcok最少128或256个thread。根据kernel需要的资源调整block。保证block的数目远大于SM的数目。