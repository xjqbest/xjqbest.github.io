---
layout: post
title:  "深度学习框架中的自动求导"
date:   2020-05-15 10:04:00
categories: DeepLearning
tags: DeepLearning
excerpt: 深度学习框架中的自动求导
---

自动求导分成两种模式，一种是 Forward Mode，另外一种是 Reverse Mode。一般的机器学习库用的后一种。

核心：链式法则

## Forward Mode

对于如下函数：

<img src="/images/autodiff/1.png" width="80%" height="80%">

转化成如上DAG（有向无环图）结构之后，我们可以很容易分步计算函数的值，并求取它每一步的导数值：

<img src="/images/autodiff/2.png" width="70%" height="70%">

上表中左半部分是从左往右每个图节点的求值结果，右半部分是每个节点对于 $$ x_1 $$ 的求导结果。


一个有n个输入（n个变量，比如embedding维度）的函数，求解函数梯度需要n遍如上计算过程。


## Reverse Mode

reverse mode在计算梯度先不考虑chain rule，最后再用 chain rule 把梯度组起来。而前者则是直接就应用 chain rule 来算梯度。


<img src="/images/autodiff/5.png" width="70%" height="70%">

后向模式的最大优势是对于诸如$$ f: \mathbb{R}^n \rightarrow \mathbb{R} $$
这样的函数，一次后向模式就可以得到全部梯度$$ \nabla f $$。
由于机器学习问题大部分都是要求标量目标函数对高维参数的梯度，因此在这样的场景下后向模式的效率比前向模式的效率要高很多


假如我们要计算的梯度的函数是

<img src="/images/autodiff/3.png" width="10%" height="10%">

那么
 - 如果 n 是相对比较大的话，用 forward 比较省计算
 - 如果 m 是相对比较大的话，用 reverse 比较省计算


### 相关链接

[http://txshi-mt.com/2018/10/04/NMT-Tutorial-3b-Autodiff/](http://txshi-mt.com/2018/10/04/NMT-Tutorial-3b-Autodiff/)

[https://blog.csdn.net/aws3217150/article/details/70214422](https://blog.csdn.net/aws3217150/article/details/70214422)

[https://www.zhihu.com/question/54554389](https://www.zhihu.com/question/54554389)