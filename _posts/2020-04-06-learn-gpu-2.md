---
layout: post
title:  "GPU学习-2"
date:   2020-04-06 10:04:00
categories: GPU
tags: GPU
excerpt: GPU学习-2
---

## 基础Cuda


一个典型的CUDA程序结构包含五个主要步骤：

 - 分配GPU空间。
 - 将数据从CPU端复制到GPU端。
 - 调用CUDA kernel来执行计算。
 - 计算完成后将数据从GPU拷贝回CPU。
 - 清理GPU内存空间。
 

CUDA编程允许程序执行在异构系统上，即CPU和GPU，二者有各自的存储空间，并由pcie总线区分开。
kernel用\_\_global\_\_注明，告诉编译器这个function将由CPU调用在GPU上执行

### cuda里线程是如何组织

<img src="/images/learn_gpu_2/1.png" width="50%" height="50%">

由一个单独的kernel启动的所有线程组成一个grid，grid中所有线程共享global memory。一个grid由许多block组成，block由许多线程组成，grid和block都可以是一维二维或者三维，上图是一个二维grid和二维block。

这里介绍几个CUDA内置变量：

 - blockIdx：block的索引，blockIdx.x表示block的x坐标, blockIdx.y表示block的y坐标,。
 - threadIdx：线程索引，同理blockIdx。
 - blockDim：block维度，上图中blockDim.x=5, blockDim.y=3.
 - gridDim：grid维度，同理blockDim。

例子：
```cpp
dim3 block(5, 3);
dim3 grid((N+block.x-1)/block.x, (N+block.y-1)/block.y);
```

CUDA kernel的调用格式为：
```cpp
kernel_name<<<grid, block>>>(argument list);
```

如下该行代码表明有grid为一维，有4个block，block为一维，每个block有8个线程，故共有4\*8=32个线程。
```cpp
kernel_name<<<4, 8>>>(argument list);
```

<img src="/images/learn_gpu_2/4.png" width="60%" height="60%">


**软件里的概念和硬件的对应关系**

<img src="/images/learn_gpu_2/7.png" width="60%" height="60%">

GPU硬件的一个核心组件是SM, SM是英文名是Streaming Multiprocessor。SM的核心组件包括CUDA核心(其实就是ALU, 如上图绿色小块就是一个CUDA核心)、共享内存、寄存器等, SM可以并发地执行成百上千个线程, 并发能力就取决于SM所拥有的资源数. 当一个kernel被执行时, 它的gird中的线程块被分配到SM上, 一个线程块只能在一个SM上被调度， SM可以调度多个线程块。

SM采用的是SIMT(Single-Instruction, Multiple-Thread, 单指令多线程)架构, 基本的执行单元是线程束(wraps), 线程束包含32个线程, 这些线程同时执行相同的指令, 但是每个线程都包含自己的指令地址计数器和寄存器状态,也有自己独立的执行路径。当一个warp空闲时，SM就可以调度驻留在该SM中另一个可用warp。在并发的warp之间切换是没什么消耗的，因为硬件资源早就被分配到所有thread和block，所以该新调度的warp的状态已经存储在SM中了。


### memory操作

<img src="/images/learn_gpu_2/3.png" width="70%" height="70%">

```cpp

cudaError_t cudaMalloc ( void** devPtr, size_t size );

// kind有四种，host与device的组合
cudaError_t cudaMemcpy ( void* dst, const void* src, size_t count,cudaMemcpyKind kind );


```

**几种类型的memory**：

 - Registers：寄存器是GPU最快的memory，kernel中没有什么特殊声明的自动变量都是放在寄存器中的。寄存器变量是每个线程私有的，一旦thread执行结束，寄存器变量就会失效。寄存器是稀有的资源。

 - Local Memory：在local memory中的变量本质上跟global memory在同一块存储区。所以，local memory有很高的latency和较低的bandwidth。一般会把较大的结构体或者数组，也就是那些可能会消耗大量register的变量放到local Memory。

 - Shared Memory：用__shared__修饰符修饰的变量存放在shared memory。因为shared memory是on-chip的，他相比localMemory和global memory来说，拥有高的多bandwidth和低很多的latency。他的使用和CPU的L1cache非常类似，但是他是programmable的。shared memory尽管在kernel里声明的，但是他的生命周期是伴随整个block，而不是单个thread。当该block执行完毕，他所拥有的资源就会被释放，重新分配给别的block。shared memory是thread交流的基本方式。同一个block中的thread通过shared memory中的数据来相互合作。获取shared memory的数据前必须先用__syncthreads()同步。

 - Global Memory：空间最大，latency最高，GPU最基础的memory。“global”指明了其生命周期。任意SM都可以在整个程序的生命期中获取其状态。

 - L1/L2: 每个SM都有一个L1 cache，所有SM共享一个L2 cache。二者都是用来缓存local和global memory的 

 - Constant Memory：Constant Memory驻留在device Memory，并且使用专用的constant cache（per-SM）。constant的范围是全局的，针对所有kernel，对于所有CC其大小都是64KB。当一个warp中所有thread都从同一个Memory地址读取数据时，constant Memory表现最好。例如，计算公式中的系数。一次读constant Memory操作会广播给所有thread知道。

 - Texture Memory：驻留在device Memory中，并且使用一个只读cache（per-SM）。texture Memory实际上也是global Memory在一块，但是他有自己专有的只读cache。thread要获取2D数据就可以使用texture Memory来达到很高的性能

下图是CPU和GPU之间传输关系图，可以看出来，CPU和GPU之间传输速度相对很差（NVLink技术能提高5~10倍），GPU和on-board Memory传输速度要快得多，所以对于编程来说，要时刻考虑减少CPU和GPU之间的数据传输。

<img src="/images/learn_gpu_2/5.png" width="45%" height="45%">

还有一种特殊的是**Pinned Memory**，由于pinned Memory能够被device直接访问（不是指不通过PCIE了，而是相对左图我们少了pageable Memory到pinned Memory这一步），所以他比pageable Memory具有相当高的读写带宽，当然像这种东西依然不能过度使用，因为这会降低pageable Memory的数量，影响整个虚拟存储性能。

<img src="/images/learn_gpu_2/6.png" width="50%" height="50%">

当Host调用异步的memcpy时，host memory必须为pinned memory。GPU无法安全的获取host的pageable Memory，因为GPU没有办法控制host OS物理上转移数据的时机。

前面说了一下cuda中的kernel与memory，基本是涵盖大部分概念，下面详细深入一下。

### warp

逻辑上，所有thread是并行的，但是，从硬件的角度来说，实际上并不是所有的thread能够在同一时刻执行。

warp是SM的基本执行单元。一个warp包含32个并行thread，这32个thread执行于SMIT模式。也就是说所有thread执行同一条指令，并且每个thread会使用各自的data执行该指令。

一个warp中的线程必然在同一个block中，如果block所含线程数目不是warp大小的整数倍，那么多出的那些thread所在的warp中，会剩余一些inactive的thread，也就是说，即使凑不够warp整数倍的thread，硬件也会为warp凑足，只不过那些thread是inactive状态，需要注意的是，即使这部分thread是inactive的，也会消耗SM资源。


什么是**Warp Divergence**

同一个warp中的所有thread必须执行相同的指令，那么如果这些线程在遇到控制流语句时，如果进入不同的分支，那么同一时刻除了正在执行的分之外，其余分支都被阻塞了，十分影响性能。这类问题就是warp divergence。

warp divergence问题只会发生在同一个warp中。

<img src="/images/learn_gpu_2/8.png" width="70%" height="70%">

为了获得最好的性能，就需要避免同一个warp存在不同的执行路径。

优化前：(分支的条件是thread的唯一ID的奇偶)
```cpp
__global__ void mathKernel1(float *c) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float a, b;
    a = b = 0.0f;
    if (tid % 2 == 0) {
        a = 100.0f;
    } else {
        b = 200.0f;
    }
    c[tid] = a + b;
}
```

优化后：(将条件改为以warp大小为步调，然后取奇偶，那么每个warp中的执行路径就一致了，最后一个warp除外)
```cpp
__global__ void mathKernel2(void) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float a, b;
    a = b = 0.0f;
    if ((tid / warpSize) % 2 == 0) {
        a = 100.0f;
    } else {
        b = 200.0f;
    }
    c[tid] = a + b;
}
```

### Resource Partitioning

一个warp的context包括三部分：程序计数器、寄存器、shared memory。

SM处理的每个warp的执行context都是on-chip的，所以context切换是没有消耗的。

每个SM有一个32位register集合，还有固定数量的shared memory，这些资源都被thread瓜分了，由于资源是有限的，所以，如果thread比较多，那么每个thread占用资源就叫少，thread较少，占用资源就较多。

当一个block或得到足够的资源时，就成为active block。block中的warp就称为active warp。active warp又可以被分为下面三类：Selected warp、Stalled warp、Eligible warp。

SM中warp调度器每个cycle会挑选active warp送去执行，一个被选中的warp称为selected warp，没被选中，但是已经做好准备被执行的称为Eligible warp，没准备好要执行的称为Stalled warp。

warp适合执行需要满足下面两个条件：32个CUDA core有空、所有当前指令的参数都准备就绪。

资源限制了驻留在SM中block的数量，如果没有足够的资源，kernel的启动就会失败。

<img src="/images/learn_gpu_2/10.png" width="70%" height="70%">

### Latency Hiding

指令从开始到结束消耗的clock cycle称为指令的latency。当每个cycle都有eligible warp被调度时，计算资源就会得到充分利用，

指令可以被区分为两种： 算数操作（10-20 cycle）、内存操作（400-800 cycles）。

下图是一个简单的执行流程，当warp0阻塞时，执行其他的warp，当warp变为eligible时从新执行。

<img src="/images/learn_gpu_2/9.png" width="70%" height="70%">


也就是说我们可以将每个指令的latency隐藏于issue其它warp的指令的过程中。

如何增加并行性：

 - Instruction-level Parallelism（ILP）：同一个thread中更多的独立指令
 - read-level Parallelism （TLP）：更多并发的eligible threads


总结一下grid和block的配置准则：

 - 保证block中thrad数目是32的倍数。（因为warp大小是32，避免inactive thread浪费sm资源）
 - 避免block太小：每个block最少128或256个thread。（每个sm上跑的block数是受限制的、shared memory频繁申请释放）
 - 根据kernel需要的资源调整block。
 - 保证block的数目远大于SM的数目。（增加并行性、足够多的warp隐藏latency）
 - 多做实验来挖掘出最好的配置。

### Synchronize

1、 System-level：等待所有host和device的工作完成（cudaError_t cudaDeviceSynchronize(void);）

2、block-level：等待device中block的所有thread执行到某个点（\_\_device\_\_ void \_\_syncthreads(void);）

### Aligned and Coalesced Access

如下图所示，global Memory的load/store要经由cache，所有的数据会初始化在DRAM，也就是物理的device Memory上，而kernel能够获取的global Memory实际上是一块逻辑内存空间。Kernel对Memory的请求都是由DRAM和SM的片上内存以128-byte和32-byte传输解决的。

<img src="/images/learn_gpu_2/11.png" width="70%" height="70%">

我们在设计代码的时候，有两个特征需要注意:

 - Aligned Memory access 对齐
 - Coalesced Memory access 连续

 L1 cache中每一行(cache line)是128bytes，这些数据映射到device Memory上的128位对齐的块。
当要获取的Memory首地址是cache line的倍数时，就是Aligned Memory Access，如果是非对齐的，就会导致浪费带宽。至于Coalesced Memory Access则是warp的32个thread请求的是连续的内存块。

下图就是很好的符合了连续和对齐原则，只有128-byte Memory传输的消耗：

<img src="/images/learn_gpu_2/12.png" width="50%" height="50%">

下图则没有遵守连续和对齐原则，有三次传输消耗发生，一次是从偏移地址0开始，一次是从偏移地址256开始，还有一次是从偏移128开始，而这次包含了大部分需要的数据，另外两次则有很多数据并不是需要的，而导致带宽浪费。

<img src="/images/learn_gpu_2/13.png" width="50%" height="50%">

**Global Memory Reads**

在SM中，数据运送是要经过三种cache/buffer，主要依赖于要获取的device Memory种类：

 - L1/L2 cache：Global Memory的load操作是否经过L1cache由算力和编译选项决定。
 - Constant cache
 - Read-only cache：该cache可以替换L1，单位是32 bytes。

**Global Memory Writes**

写操作相对要简单的多，L1压根就不使用了。数据只会cache在L2中，所以写操作也是以32bytes为单位的。Memory transaction一次可以是一个、两个或四个segment。

连续且对齐，只需要一次4-segment的传输：

<img src="/images/learn_gpu_2/14.png" width="50%" height="50%">

下图是离散的情况，会由三次1-segment传输完成。

<img src="/images/learn_gpu_2/15.png" width="50%" height="50%">

下图是对齐且地址在一个连续的64-byte范围内的情况，由一次2-segment传输完成

<img src="/images/learn_gpu_2/16.png" width="50%" height="50%">

调节device Memory带宽利用性能时，主要是力求达到下面两个目标：

 - （连续且对齐）Aligned and Coalesced Memory accesses that reduce wasted bandwidth
 - （并发访问内存）Sufficient concurrent Memory operations to hide Memory latency

什么是**Unrolling Techniques**: 展开循环可以增加更多的独立的Memory操作。

### shared memory

global memory就是一块很大的on-board memory，并且有很高的latency。而shared memory正好相反，是一块很小，低延迟的on-chip memory，比global memory拥有高得多的带宽。我们可以把他当做可编程的cache。

当一个block开始执行时，GPU会分配其一定数量的shared memory，这个shared memory的地址空间会由block中的所有thread 共享。shared memory是划分给SM中驻留的所有block的，也是GPU的稀缺资源。所以，使用越多的shared memory，能够并行的active block就越少。

**Memory Banks**：为了获得高带宽，shared Memory被分成32（对应warp中的thread）个相等大小的内存块，他们可以被同时访问。如果warp访问shared Memory，对于每个bank只访问不多于一个内存地址，那么只需要一次内存传输就可以了，否则需要多次传输，因此会降低内存带宽的使用率。

**Bank Conflict**：当多个地址请求落在同一个bank中就会发生bank conflict，从而导致请求多次执行。（举例，如果warp中的32个thread都访问了同一个bank中的不同位置，那就是32次单独的请求，而不是同时访问了。）

下图是最优情况的访问图示：

<img src="/images/learn_gpu_2/17.png" width="50%" height="50%">

下图一种随机访问，同样没有conflict：

<img src="/images/learn_gpu_2/18.png" width="50%" height="50%">

下图则是某些thread访问到同一个bank的情况，这种情况有两种行为：  
 - Conflict-free broadcast access if threads access the same address within a bank
 - Bank conflict access if threads access different addresses within a bank

<img src="/images/learn_gpu_2/19.png" width="50%" height="50%">

**Volatile**：使用该修饰符后，编译器就会认为该变量可能在某一时刻被别的thread改变，如果使用cache优化的话，得到的值就缺乏时效，因此使用volatile强制每次都到global 或者shared Memory中去读取其绝对有效值。

## cuda stream


