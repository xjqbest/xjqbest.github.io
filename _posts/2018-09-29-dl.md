---
layout: post
title:  "深度学习积累"
date:   2018-09-28 12:00:00
categories: DeepLearning MachineLearning
tags: DeepLearning MachineLearning
excerpt: 
---

## BatchNorm

解决的问题：每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难。由于训练数据和测试数据存在分布的差异性，给网络的泛化性和训练速度带来了影响。

假设我们的数据分布如a所示，参数初始化一般是0均值，和较小的方差，此时拟合的y=wx+b如b图中的橘色线，经过多次迭代后，达到紫色线，此时具有很好的分类效果，但是如果我们将其归一化到0点附近，显然会加快训练速度，如此我们更进一步的通过变换扩大数据之间的相对差异性，那么就更容易区分了。

<img src="/images/dl/1.png" width="65%" height="65%">

之所以称之为batchnorm是因为归一化的数据是一个batch的，
假设有m个输入$$ X ={ x_{1...m} } $$。batchnorm步骤如下：

1. 求出该batch数据x的均值: $$ \mu_X = \frac{1}{m} \sum_{i=1}^{m} x_i $$
2. 求出该batch数据x的方差: $$ \sigma_{X}^{2} = \frac{1}{m}\sum_{i=1}{m}(x_i-\mu_{X})^2 $$
3. 归一化: 
\begin{align}
 \hat{x_i} = \frac{x_i-\mu_X}{\sqrt{\mu_X^2+\epsilon}} 
\end{align}
4. 引入缩放和平移变量$$ \gamma $$ 和 $$ \beta $$: 
\begin{align}
 y_i = \gamma \hat x_i + \beta 
\end{align}

如果$$ \gamma $$ 和 $$ \beta $$分别等于该batch的方差和均值，
那么$$ y_i $$就还原到归一化前的x了，相当于batchnorm没有起作用。
因此这两个参数的作用就是每一次数据经过归一化后还保留的有学习来的特征，
同时又能完成归一化这个操作，加速训练。

在求取某个流数据的平均值的时候，常用的一种方法是滑动平均法，
也就是使用系数$$ \alpha $$来做平滑滤波:

$$ S_t = \alpha Y_t + (1-\alpha) S_{t-1} $$

等价于：

$$ S_t = \frac{\text{WeightedSum}_n}{\text{WeightedCount}_n} $$

其中：

$$ \text{WeightedSum}_n = Y_t + (1-\alpha) \text{WeightedSum}_{n-1} $$

$$ \text{WeightedCount}_n = 1 + (1-\alpha) \text{WeightedCount}_{n-1} $$

Caffe中BN的实现中使用了三个blob，$$ blobs[0] $$和$$ blobs[1] $$
中存储的实际是$$ \text{WeightedSum}_n $$，
而$$ blobs[2] $$中存储的是$$ \text{WeightedCount}_n $$。
所以，真正的mean和var是两者相除的结果。即：
```
mean = blobs_[0] / blobs_[2]
var = blobs_[1] / blobs_[2]
```

