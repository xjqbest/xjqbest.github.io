---
layout: post
title:  "深度学习积累"
date:   2018-09-28 12:00:00
categories: DeepLearning MachineLearning
tags: DeepLearning MachineLearning
excerpt: 
---

## BatchNorm

解决的问题：每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难。由于训练数据和测试数据存在分布的差异性，给网络的泛化性和训练速度带来了影响。

假设我们的数据分布如a所示，参数初始化一般是0均值，和较小的方差，此时拟合的y=wx+b如b图中的橘色线，经过多次迭代后，达到紫色线，此时具有很好的分类效果，但是如果我们将其归一化到0点附近，显然会加快训练速度，如此我们更进一步的通过变换扩大数据之间的相对差异性，那么就更容易区分了。

<img src="/images/dl/1.png" width="65%" height="65%">

之所以称之为batchnorm是因为归一化的数据是一个batch的，
假设有m个输入$$ X ={ x_{1...m} } $$。batchnorm步骤如下：

1. 求出该batch数据x的均值: $$ \mu_X = \frac{1}{m} \sum_{i=1}^{m} x_i $$
2. 求出该batch数据x的方差: $$ \sigma_{X}^{2} = \frac{1}{m}\sum_{i=1}{m}(x_i-\mu_{X})^2 $$
3. 归一化: 
\begin{align}
 \hat{x_i} = \frac{x_i-\mu_X}{\sqrt{\mu_X^2+\epsilon}} 
\end{align}
4. 引入缩放和平移变量$$ \gamma $$ 和 $$ \beta $$: 
\begin{align}
 y_i = \gamma \hat x_i + \beta 
\end{align}

如果$$ \gamma $$ 和 $$ \beta $$分别等于该batch的方差和均值，
那么$$ y_i $$就还原到归一化前的x了，相当于batchnorm没有起作用。
因此这两个参数的作用就是每一次数据经过归一化后还保留的有学习来的特征，
同时又能完成归一化这个操作，加速训练。

在求取某个流数据的平均值的时候，常用的一种方法是滑动平均法，
也就是使用系数$$ \alpha $$来做平滑滤波:

$$ S_t = \alpha Y_t + (1-\alpha) S_{t-1} $$

等价于：

$$ S_t = \frac{\text{WeightedSum}_n}{\text{WeightedCount}_n} $$

其中：

$$ \text{WeightedSum}_n = Y_t + (1-\alpha) \text{WeightedSum}_{n-1} $$

$$ \text{WeightedCount}_n = 1 + (1-\alpha) \text{WeightedCount}_{n-1} $$

Caffe中BatchNorm的实现中使用了三个blob，$$ blobs[0] $$和$$ blobs[1] $$
中存储的实际是$$ \text{WeightedSum}_n $$，
而$$ blobs[2] $$中存储的是$$ \text{WeightedCount}_n $$。
所以，真正的mean和var是两者相除的结果。即：
```
mean = blobs_[0] / blobs_[2]
var = blobs_[1] / blobs_[2]
```


Caffe中BatchNorm实现的是归一化 ,Scale实现的是平移和缩放。并且对于BatchNorm，计算的是$$batchsize*width*height$$个所有像素点的均值和方差，
也就是除了channel外所有维度计算均值和方差。

一个输出特征图上的每个点，都是用一个均值和方差做归一化。
最终的结果的维数等于卷积核个数，也即某一个卷积核得到的batch_size个输出上，所有像素的均值和方差。

## DropOut

<img src="/images/dl/2.png" width="50%" height="50%">

左边是我们常见的FC Layer, 右边是使用了dropout之后的效果。

首先设定一个dropout ratio，范围在(0, 1)之间，表示在训练的Forward阶段需要去掉的神经元的比例，
这些神经元不参与前向和后向传播。而在预测的时候, 使用全部的连接，但是要乘以1-ratio.

caffe实现是训练时除以1-ratio，这样预测的时候就不用乘以1-ratio了。

caffe中的实现如下,其中
```cpp
template <typename Dtype>
void DropoutLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,
    const vector<Blob<Dtype>*>& top) {
  const Dtype* bottom_data = bottom[0]->cpu_data();
  Dtype* top_data = top[0]->mutable_cpu_data();
  unsigned int* mask = rand_vec_.mutable_cpu_data();
  const int count = bottom[0]->count();
  if (this->phase_ == TRAIN) {
    // Create random numbers
    caffe_rng_bernoulli(count, 1. - threshold_, mask);
    for (int i = 0; i < count; ++i) {
      top_data[i] = bottom_data[i] * mask[i] * scale_;
    }
  } else {
    caffe_copy(bottom[0]->count(), bottom_data, top_data);
  }
}
其中：
threshold_ = this->layer_param_.dropout_param().dropout_ratio();
scale_ = 1. / (1. - threshold_);

template <typename Dtype>
void DropoutLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
    const vector<bool>& propagate_down,
    const vector<Blob<Dtype>*>& bottom) {
  if (propagate_down[0]) {
    const Dtype* top_diff = top[0]->cpu_diff();
    Dtype* bottom_diff = bottom[0]->mutable_cpu_diff();
    if (this->phase_ == TRAIN) {
      const unsigned int* mask = rand_vec_.cpu_data();
      const int count = bottom[0]->count();
      for (int i = 0; i < count; ++i) {
        bottom_diff[i] = top_diff[i] * mask[i] * scale_;
      }
    } else {
      caffe_copy(top[0]->count(), top_diff, bottom_diff);
    }
  }
}
```

dropout的好处是不仅可以防止over-fitting，还有model-ensemble的作用，每次forward都是不同的子网络，最终的模型可以看作对这些模型取平均。

不适用的情况主包括宽度太窄的网络、训练集太小的网络等。

## concat

caffe concat layer实现了两个或多个layer进行拼接，支持num和channel维度的拼接。

np中的例子：
```python
a = np.array([[1, 2], [3, 4]]) 
b = np.array([[5, 6]])
np.concatenate((a, b), axis=0)  # 这里的axis=0的表示按照行进行合并

输出
array([[1, 2],
       [3, 4],
       [5, 6]])

a = np.array([[1, 2], [3, 4]]) 
b = np.array([[5, 6]])
np.concatenate((a, b.T), axis=1)  # 这里的axis=1的表示按照列进行合并

输出
array([[1, 2, 5],
       [3, 4, 6]])
```

## 参考链接

[http://shuokay.com/2016/06/14/dropout/](http://shuokay.com/2016/06/14/dropout/)

