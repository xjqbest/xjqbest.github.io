---
layout: post
title:  "深度学习积累"
date:   2018-09-28 12:00:00
categories: DeepLearning MachineLearning
tags: DeepLearning MachineLearning
excerpt: 
---

## BatchNorm

解决的问题：每一次参数迭代更新后，上一层网络的输出数据经过这一层网络计算后，数据的分布会发生变化，为下一层网络的学习带来困难。由于训练数据和测试数据存在分布的差异性，给网络的泛化性和训练速度带来了影响。

假设我们的数据分布如a所示，参数初始化一般是0均值，和较小的方差，此时拟合的y=wx+b如b图中的橘色线，经过多次迭代后，达到紫色线，此时具有很好的分类效果，但是如果我们将其归一化到0点附近，显然会加快训练速度，如此我们更进一步的通过变换扩大数据之间的相对差异性，那么就更容易区分了。

<img src="/images/dl/1.png" width="65%" height="65%">

之所以称之为batchnorm是因为归一化的数据是一个batch的，
假设有m个输入$$ X ={ x_{1...m} } $$。batchnorm步骤如下：

1. 求出该batch数据x的均值: $$ \mu_X = \frac{1}{m} \sum_{i=1}^{m} x_i $$
2. 求出该batch数据x的方差: $$ \sigma_{X}^{2} = \frac{1}{m}\sum_{i=1}^{m}(x_i-\mu_{X})^2 $$
3. 归一化: 
\begin{align}
 \hat{x_i} = \frac{x_i-\mu_X}{\sqrt{\mu_X^2+\epsilon}} 
\end{align}
4. 引入缩放和平移变量$$ \gamma $$ 和 $$ \beta $$: 
\begin{align}
 y_i = \gamma \hat x_i + \beta 
\end{align}

如果$$ \gamma $$ 和 $$ \beta $$分别等于该batch的方差和均值，
那么$$ y_i $$就还原到归一化前的x了，相当于batchnorm没有起作用。
因此这两个参数的作用就是每一次数据经过归一化后还保留的有学习来的特征，
同时又能完成归一化这个操作，加速训练。

在求取某个流数据的平均值的时候，常用的一种方法是滑动平均法，
也就是使用系数$$ \alpha $$来做平滑滤波:

$$ S_t = \alpha Y_t + (1-\alpha) S_{t-1} $$

等价于：

$$ S_t = \frac{\text{WeightedSum}_n}{\text{WeightedCount}_n} $$

其中：

$$ \text{WeightedSum}_n = Y_t + (1-\alpha) \text{WeightedSum}_{n-1} $$

$$ \text{WeightedCount}_n = 1 + (1-\alpha) \text{WeightedCount}_{n-1} $$

Caffe中BatchNorm的实现中使用了三个blob，$$ blobs[0] $$和$$ blobs[1] $$
中存储的实际是$$ \text{WeightedSum}_n $$，
而$$ blobs[2] $$中存储的是$$ \text{WeightedCount}_n $$。
所以，真正的mean和var是两者相除的结果。即：
```
mean = blobs_[0] / blobs_[2]
var = blobs_[1] / blobs_[2]
```


Caffe中BatchNorm实现的是归一化 ,Scale实现的是平移和缩放。并且对于BatchNorm，计算的是$$batchsize*width*height$$个所有像素点的均值和方差，
也就是除了channel外所有维度计算均值和方差。

一个输出特征图上的每个点，都是用一个均值和方差做归一化。
最终的结果的维数等于卷积核个数，也即某一个卷积核得到的batch_size个输出上，所有像素的均值和方差。

## DropOut

<img src="/images/dl/2.png" width="50%" height="50%">

左边是我们常见的FC Layer, 右边是使用了dropout之后的效果。

首先设定一个dropout ratio，范围在(0, 1)之间，表示在训练的Forward阶段需要去掉的神经元的比例，
这些神经元不参与前向和后向传播。而在预测的时候, 使用全部的连接，但是要乘以1-ratio.

caffe实现是训练时除以1-ratio，这样预测的时候就不用乘以1-ratio了。

caffe中的实现如下,其中
```cpp
template <typename Dtype>
void DropoutLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,
    const vector<Blob<Dtype>*>& top) {
  const Dtype* bottom_data = bottom[0]->cpu_data();
  Dtype* top_data = top[0]->mutable_cpu_data();
  unsigned int* mask = rand_vec_.mutable_cpu_data();
  const int count = bottom[0]->count();
  if (this->phase_ == TRAIN) {
    // Create random numbers
    caffe_rng_bernoulli(count, 1. - threshold_, mask);
    for (int i = 0; i < count; ++i) {
      top_data[i] = bottom_data[i] * mask[i] * scale_;
    }
  } else {
    caffe_copy(bottom[0]->count(), bottom_data, top_data);
  }
}
其中：
threshold_ = this->layer_param_.dropout_param().dropout_ratio();
scale_ = 1. / (1. - threshold_);

template <typename Dtype>
void DropoutLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
    const vector<bool>& propagate_down,
    const vector<Blob<Dtype>*>& bottom) {
  if (propagate_down[0]) {
    const Dtype* top_diff = top[0]->cpu_diff();
    Dtype* bottom_diff = bottom[0]->mutable_cpu_diff();
    if (this->phase_ == TRAIN) {
      const unsigned int* mask = rand_vec_.cpu_data();
      const int count = bottom[0]->count();
      for (int i = 0; i < count; ++i) {
        bottom_diff[i] = top_diff[i] * mask[i] * scale_;
      }
    } else {
      caffe_copy(top[0]->count(), top_diff, bottom_diff);
    }
  }
}
```

dropout的好处是不仅可以防止over-fitting，还有model-ensemble的作用，每次forward都是不同的子网络，最终的模型可以看作对这些模型取平均。

不适用的情况主包括宽度太窄的网络、训练集太小的网络等。

## concat

caffe concat layer实现了两个或多个layer进行拼接，支持num和channel维度的拼接。

np中的例子：
```python
a = np.array([[1, 2], [3, 4]]) 
b = np.array([[5, 6]])
np.concatenate((a, b), axis=0)  # 这里的axis=0的表示按照行进行合并

# 输出
array([[1, 2],
       [3, 4],
       [5, 6]])

a = np.array([[1, 2], [3, 4]]) 
b = np.array([[5, 6]])
np.concatenate((a, b.T), axis=1)  # 这里的axis=1的表示按照列进行合并

# 输出
array([[1, 2, 5],
       [3, 4, 6]])
```

## RNN 、 LSTM

RNN：前向传播时，在第t时刻的输出$$ h_t $$由$$ h_{t-1} $$ 和
输入$$ x_t $$共同决定，反向传播时，首先求出每个时刻的$$ h_t $$的梯度，
进而参数的剃度也可求出。

<img src="/images/dl/3.png" width="60%" height="60%">

<img src="/images/dl/4.png" width="60%" height="60%">

LSTM：关键是Cell State，上面承载的信息可以很容易地流过而不改变。

<img src="/images/dl/5.png" width="60%" height="60%">

### LSTM分步详解

（1）第一步是决定我们将要从Cell State中扔掉哪些信息。
遗忘门观察 $$ h_{t-1} $$ 和 $$ x_t $$，对于状态 $$ C_{t-1} $$ 中的每一个元素，输出一个0~1之间的数。1表示“完全保留该信息”，0表示“完全丢弃该信息”。

<img src="/images/dl/6.png" width="65%" height="65%">

（2）第二步是决定我们将会把哪些新信息存储到Cell State中。
首先，有一个叫做输入门(Input Gate)的Sigmoid层决定我们要更新哪些信息。
接下来一个tanh层创造了一个新的候选值，$$ \hat C $$,
该值可能被加入到元胞状态中。在下一步中，我们将会把这两个值组合起来用于更新Cell State。

<img src="/images/dl/7.png" width="65%" height="65%">

（3）执行状态更新 $$ C_t $$。把旧状态 $$ C_{t-1} $$ 乘以 $$ f_t $$，
忘掉我们已经决定忘记的内容。然后我们再加上 $$ i_t * \hat {C_t} $$。

<img src="/images/dl/8.png" width="65%" height="65%">


（4）第四步是决定最终的输出。首先我们建立一个Sigmoid层的输出门(Output Gate)
，来决定我们将输出Cell State的哪些部分。然后我们将Cell State通过tanh之后
（使得输出值在-1到1之间），与输出门相乘，这样我们只会输出我们想输出的部分。

<img src="/images/dl/9.png" width="65%" height="65%">

[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

[循环神经网络(RNN)模型与前向反向传播算法](https://www.cnblogs.com/pinard/p/6509630.html)

[LSTM模型与前向反向传播算法](https://www.cnblogs.com/pinard/p/6519110.html)

## 参考链接

[http://shuokay.com/2016/06/14/dropout/](http://shuokay.com/2016/06/14/dropout/)

