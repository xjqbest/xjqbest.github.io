---
layout: post
title:  "学习卷积网络"
date:   2018-09-04 12:00:00
categories: MachineLearning
tags: MachineLearning
excerpt: CNN
---

# 简介

卷积神经网络（convolutional neural network, CNN），是一种专门用来处理具有类似网格结构的数据的
神经网络。例如时间序列数据（可以认为是在时间轴上有规律地采样形成的一维网格）和图像数据（可以看作是二维的像素网格）。

## motivation

卷积运算通过三个重要的思想来帮助改进机器学习系统： 
稀疏交互（sparse interactions，也叫做稀疏连接或者稀疏权重）、参数共享（parameter sharing）、
等变表示（equivariant representa-tions）。

### 稀疏连接

相比矩阵乘法，连接是稀疏的，减少了计算量。  
设输入大小为m，输出大小为n，卷积核大小为k，则计算量从$$ O(m * n) $$减小到$$ O(k * n) $$

<img src="/images/cnn/2.png" width="50%" height="50%">

### 参数共享

卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。

如上图，我们只需要学习$$ k = 3 $$个参数，而不是$$ m * n $$个。

### 等变

对于卷积，参数共享也使得神经网络层具有对平移等变的性质。

等变的定义：如果一个函数满足输入改变，输出也以同样的方式改变这一性质，我们就说
它是等变的。如果函数$$ f(x) $$与$$ g(x) $$满足$$ f(g(x)) = g(f(x)) $$，
我们就说$$ f(x) $$对于变换$$ g $$具有等变性。

例子：对于2维的图像，卷积产生了一个2维的输出来表明某些特征在输入中出现的位置。如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。

## 卷积的变体

卷积有两种变体：
1. 非共享卷积（也叫局部连接）:它和具有一个核的卷积运算很像，但并不横跨位置来共享参数。
2. 平铺卷积（tiled convolution）：对卷积层和局部连接层进行了折衷，我们学习一组核使得当我们在空间移动时它们可以循环利用。

下图是基本的卷积：

<img src="/images/cnn/6.png" width="40%" height="40%">

下图分别是局部连接、平铺卷积、全连接：

<img src="/images/cnn/5.png" width="40%" height="40%">

可以看出：
1. 局部连接与卷积区别在于不共享参数，全连接层类似于局部连接层，它的每条边都有其自身的参数。
2. 平铺卷积有t个不同的核。这里我们说明t = 2 的情况。其中一个
核具有标记为“a’’ 和“b’’ 的边，而另一个具有标记为“c’’ 和“d’’ 的边。每当我们在输出中右移一
个像素后，我们使用一个不同的核。这意味着，与局部连接层类似，输出中的相邻单元具有不同的
参数。与局部连接层不同的是，在我们遍历所有可用的t 个核之后，我们循环回到了第一个核。


# 如何计算卷积

<img src="/images/cnn/1.gif" width="50%" height="50%">

卷积计算时候，有三种类型：
1. 有效（valid）卷积：无论怎样都不使用零填充，并且卷积核只允许访问那些图像中能够完全包含整个核的位置。
2. 相同（same）卷积：只进行足够的零填充来保持输出和输入具有相同的大小。
3. 全（full）卷积：进行了足够多的零填充使得每个像素在每个方向上恰好被访问了k次，最终输出图像的宽度为m+k-1。

通常零填充的最优数量（对于测试集的分类正确率）处于“有效卷积” 和“相同卷积” 之间的某个位置。

卷积网络的一个优点是它们还可以处理具有可变的空间尺度的输入，核依据输入的大小简单地被使用不同次，并且卷积运算的输出也相应地放缩。

# 可分离卷积

## 原理

卷积等效于使用傅立叶变换将输入与核都转换到频域、执行两个信号的逐点相
乘，再使用傅立叶逆变换转换回时域。对于某些问题的规模，这种算法可能比离散
卷积的朴素实现更快。

## 例子

假设有一个3×3大小的卷积层，其输入通道为16、输出通道为32。具体为，32个3×3大小的卷积核会遍历16个通道中的每个数据，从而产生16×32=512个特征图谱。进而通过叠加每个输入通道对应的特征图谱后融合得到1个特征图谱。最后可得到所需的32个输出通道。

针对这个例子应用深度可分离卷积，用16个3×3大小的卷积核分别遍历16通道的数据，得到了16个特征图谱。在融合操作之前，接着用32个1×1大小的卷积核遍历这16个特征图谱，进行相加融合。这个过程使用了16×3×3+16×32×1×1=656个参数，远少于上面的16×32×3×3=4608个参数。


# 池化

池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。
例如， 最大池化（max pooling）函数给出相邻矩形区域内的最大值，mean pooling给出相邻矩形区域内的平均值。

在卷积神经网络中，池化层往往跟在卷积层的后面，池化层的作用有两个： 
1. 降低卷积层输出的特征向量的维度。 
2. 减少过拟合现象

<img src="/images/cnn/3.png" width="60%" height="60%">

# 补充

## 随机或者无监督

1. 随机：随机过滤器经常在卷积网络中表现得出乎意料得好。
2. 手动设计：例如设置每个核在一个特定的方向或尺度来检测边缘
3. 无监督：例如用k均值聚类算法应用于小图像块，然后每个中心作为卷积核。

一个中间的方法：使用贪心逐层预训练，单独训练第一层，然后一次性地从第一层提取所有特征，之后用那些特征单独训练第二层，以此类推。

## 卷积深度信念网络

待补充

## 分组卷积

待补充

## Resnet残差网络

待补充

## 可变形卷积核

待补充

## SEnet

待补充

## ShuffleNet

待补充

## MobileNet

待补充


# caffe中的卷积层

```
message ConvolutionParameter {
  optional uint32 num_output = 1; // The number of outputs for the layer
  optional bool bias_term = 2 [default = true]; // whether to have bias terms

  // Pad, kernel size, and stride are all given as a single value for equal
  // dimensions in all spatial dimensions, or once per spatial dimension.
  repeated uint32 pad = 3; // The padding size; defaults to 0
  repeated uint32 kernel_size = 4; // The kernel size
  repeated uint32 stride = 6; // The stride; defaults to 1
  // Factor used to dilate the kernel, (implicitly) zero-filling the resulting
  // holes. (Kernel dilation is sometimes referred to by its use in the
  // algorithme à trous from Holschneider et al. 1987.)
  repeated uint32 dilation = 18; // The dilation; defaults to 1

  // For 2D convolution only, the *_h and *_w versions may also be used to
  // specify both spatial dimensions.
  optional uint32 pad_h = 9 [default = 0]; // The padding height (2D only)
  optional uint32 pad_w = 10 [default = 0]; // The padding width (2D only)
  optional uint32 kernel_h = 11; // The kernel height (2D only)
  optional uint32 kernel_w = 12; // The kernel width (2D only)
  optional uint32 stride_h = 13; // The stride height (2D only)
  optional uint32 stride_w = 14; // The stride width (2D only)

  optional uint32 group = 5 [default = 1]; // The group size for group conv

  optional FillerParameter weight_filler = 7; // The filler for the weight
  optional FillerParameter bias_filler = 8; // The filler for the bias
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 15 [default = DEFAULT];

  // The axis to interpret as "channels" when performing convolution.
  // Preceding dimensions are treated as independent inputs;
  // succeeding dimensions are treated as "spatial".
  // With (N, C, H, W) inputs, and axis == 1 (the default), we perform
  // N independent 2D convolutions, sliding C-channel (or (C/g)-channels, for
  // groups g>1) filters across the spatial axes (H, W) of the input.
  // With (N, C, D, H, W) inputs, and axis == 1, we perform
  // N independent 3D convolutions, sliding (C/g)-channels
  // filters across the spatial axes (D, H, W) of the input.
  optional int32 axis = 16 [default = 1];

  // Whether to force use of the general ND convolution, even if a specific
  // implementation for blobs of the appropriate number of spatial dimensions
  // is available. (Currently, there is only a 2D-specific convolution
  // implementation; for input blobs with num_axes != 2, this option is
  // ignored and the ND implementation will be used.)
  optional bool force_nd_im2col = 17 [default = false];
}
```

# 相关链接

[https://blog.csdn.net/v_JULY_v/article/details/51812459](https://blog.csdn.net/v_JULY_v/article/details/51812459)

[https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo](https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo)

[https://zhuanlan.zhihu.com/p/28749411](https://zhuanlan.zhihu.com/p/28749411)