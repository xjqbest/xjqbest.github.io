---    
layout: post
title:  "large scale knn"
date:   2018-07-23 00:00:00
categories: MachineLearning
tags: MachineLearning
excerpt: 
---

# 1 Overview

K近邻检索有很多种算法：

1. 暴力搜索。遍历所有样本空间，然后统一排序。算法复杂度太高，当数据量非常大时，检索几乎不可能。
2. kd树/vp树。按照特征进行空间划分，存在回溯。当数据量较少时，效率较高，但当数据量非常大时，容易退化成暴力搜索。
3. 哈希算法。比如LSH(Locality Sensitive Hashing），
在整个特征空间做聚类，并通过hash值海明距离缩小检索范围。中等规模数据量时比较高效。
4. 矢量量化方法，即vector quantization。在矢量量化编码中，关键是码本的建立和码字搜索算法。比如常见的聚类算法Kmeans，就是一种矢量量化方法。而在相似搜索中，向量量化方法又以PQ方法最为典型。在超大规模数据量下效率和精度都比较高。

# 2 Large Scale KNN

## 2.1 Spherical Hashing

### 2.1.1 总体思想

对于超大规模的已知样本，可以在样本空间定义c个超球体，超球体的区域可以重叠，只有落在相同超球体内的样本才会被聚成一类，放进对应的桶内。查询时只需要找到待估计样本对应的桶，并在该桶内查找K个邻居，这样需要搜索的样本量大大减少了，因此查询效率可以得到极大的提升。

### 2.1.2 桶的表示

桶可以表示成二进制编码形式，编码长度为超球体个数，因此c个超球体可以编码得到$$ 2^c $$个桶。

### 2.1.3 样本分桶

每个超球体代表一个哈希函数，当样本与球心的距离大于等于球半径时，哈希函数的结果为1，否则为0。对于c个这样哈希函数，样本可以被编码成c位的二进制码，这个二进制码就是这个样本对应的桶。

每个超球都有两个参数，球心与球半径。定义

\begin{align}
o_i&=\sum_x{I(h_i(x)=1)} \\\
o_{i,j}&=\sum_x{I(h_i(x)=1,h_j(x)=1)}
\end{align}

其中$$ o_i $$表示落在第$$i$$个超球外的样本数，$$o_{i,j}$$表示同时落在第$$i$$和第$$j$$个超球外的样本数，
$$I()$$表示0-1函数，当表达式为真时$$I()=1$$，否则$$I()=0$$。

为了使得每个桶分得的样本数大致相同，理想的超球需要满足两个条件：

- 均匀划分数据：样本落在超球内的概率$$P(h_i(x)=1)=\frac{1}{2}$$，即$$o_i=\frac{N}{2}$$，$$N$$为总样本数

- 球之间两两独立：
$$P(h_i(x)=1, h_j(x)=1)=P(h_i(x)=1)P(h_j(x)=1)=\frac{1}{4}$$，即$$o_{i,j}=\frac{N}{4}$$。

定义损失函数$$L=\sum_{i,j} \vert o_{i,j}-\frac{N}{4} \vert $$，优化目标为最小化$$L$$。

算法流程：

(1) 首先随机初始化c个球心
$$p_i,i=0,1,…,c$$，计算每个样本到球心的距离，将所有样本到该球心距离的中位数作为超球半径，使其满足均匀划分数据条件。

(2)  
loop:  
1. 计算落在任意两个超球外的样本数$$o_{i,j}$$，使用下列公式计算每个球心的偏移，
$$f_m(n)$$表示m和n两个超球之间计算得到的超球m的偏移。
\begin{align}
f_i(j)&=\frac{1}{2}\frac{o_{i,j}-m/4}{m/4}(p_i-p_j) \\\
f_j(i)&=\frac{1}{2}\frac{o_{i,j}-m/4}{m/4}(p_j-p_i)
\end{align}
累加每个球心的偏移，并更新球心。
$$ p_i=p_i + \sum\limits_{j \neq i}^{c}{f_i(j)} $$  
2. 重新计算每个超球的半径

until:  
如果$$L$$的值满足以下三种情况则认为算法收敛，退出迭代：  
a、$$L=0$$. 
b、$$L > L'$$, $$L'$$为上一次迭代计算的损失值  
c、$$\vert \frac{L-L'}{L'} \vert < \epsilon $$, $$\epsilon$$取值$$10^{-5}$$  

### 2.1.4 并行化实现

#### 调整半径——均匀划分数据

1. map计算每个数据点与C个球心的相似度  
2. reduce，得到C个中位数作为半径

可以采用直方图法加速和节约内存

#### 调整球心——两两独立

1. map对每条数据计算C个哈希函数，取其中哈希值为1的哈希函数ID两两配对，作为输出流
2. reduce得到任意两个哈希函数i和j对应的$$ o_{i,j} $$ (其实就是统计数目)
3. 得到所有的$$ o_{i,j} $$后就可以计算出所有的$$ f_i(j) $$
4. 累加每个球心的偏移，并更新球心

### 2.1.4 查询

1. 将query转为二进制编码（即计算与C个球心的距离）
2. 用二进制编码对应的桶中的数据计算堆中的top K
3. 如果堆大小等于K，则结束，否则进入下一步
4. 按照hamming distance从小到大遍历桶，直到堆的大小等于K。

与一般的hamming distance不同，我们加了个分母，公式变为如下。  
这样的好处是让在同一桶中的字符串距离更小，比如
`110000与001100`  和`110000与111111`，一般的hamming distance计算出来都是4，但是如下计算`110000与111111`更小。

x和y为两个二进制串:
\begin{align}
d=\frac{(x异或y)中的1的数量}{(x\&y)中的1的数量}
\end{align}

## 2.2 Product Quantization

### 2.2.1 矢量量化方法

矢量量化方法，即Vector Quantization。在矢量量化编码中，关键是码本的建立和码字搜索算法。

量化的目的是压缩向量的表示空间，量化会带来一定程度的信息损失。

对于一个D维的向量$$ x\in R^D $$，
可以通过量化的方法映射成另一个向量$$ q(x)\in C,  C = \{c_{i};i\in I\}, I=\{0,1,2,...,k-1\} $$  
q()就是一个量化器，映射向量$$c_i$$叫做中心点（centroids），映射向量集合C是一个大小为k的码本。

这里的量化器本质就是包含一个Kmeans聚类，$$c_i$$就是第i个类簇的中心，原始的向量x则可以用对应的类中心来表示，
而量化的意思是，C是所有类中心的集合，那原始的向量x也可以用类中心在集合中的索引来表示，用索引表示就是x对应的量化编码。

对于每一个原始向量，保存索引至少需要$$ log_2k $$ bits的内存，k为集合C的大小。

### 2.2.2 Product Quantization

为了保证检索精度，压缩后的向量空间往往不能太小，而过大的向量空间又可能会导致存不下。

比如有一个128维的向量，一个量化器要生成64bits的编码，也就是类中心个数为$$k=2^{64}$$个。
对于Vector Quantization来说，使用kmeans进行$$2^{64}$$个类中心的聚类几乎是不可能的，
而且内存也无法装下$$2^{64}$$ * D大小的类中心，其中D为类中心向量的维度。

Product Quantization是在Vector Quantization的基础上划分子集，并用子集的笛卡尔积来表示量化后的向量空间的方法，
因此叫做Product Quantization。

对于D维的向量x，将x划分成m个$$\frac Dm $$维子向量，记作$$ u_j, 1 \le j \le m $$。
对于每个子向量$$u_j$$都有一个对应的量化器$$q_j$$和码本$$ C_j = \{c_{j,i}; i \in I_j\}$$，码本大小为$$k_j$$。

<img src="/images/large_scale_knn/1.png" width="60%" height="60%">

我们将x中每个子向量的类中心进行拼接，作为x最终类中心。
因此在整个Product Quantization算法中，我们总共需要得到$$\sum_{j=0}^{m}{k_{j}}$$个子向量的类中心，
但是却可以通过子向量码本的笛卡尔积生成码本$$ C=C_{1} \times C_{2} \times ... \times C_{m} $$，
码本C的大小为$$ k=k_{1}\times k_{2} \times ... \times k_{m} $$。

我们假设$$m=8$$，$$1 \le j \le m$$，$$k_j$$=256，
我们需要使用$$8 \times log_2{256}=64$$ bits对x进行量化编码，码本C的大小是$$ 256^8=2^{64} $$，
但只需要存储$$8\times 256=2048$$个子向量类中心的码本，节省了大量的存储开销。

下面主要介绍两种使用PQ量化编码计算query向量x和离线数据向量y欧式距离的方法，一种是对称的距离计算方法（Symmetric distance computation），另一种是非对称的距离计算方法（Asymmetric distance computation）。

#### 2.2.2.1 SDC

在对称计算方法中，向量x和y都用各自的量化类中心q(x)和q(y)来表示，x和y的距离d(x,y)可以用下面公式近似计算。 

$$ d^{\star}(x, y) = d(q(x), q(y)) = \sqrt{\sum_{j}^m{d(q_{j}(u_{j}(x)), q_{j}(u_{j}(y)))^2}} $$

可以将每个子向量码本中类中心的两两距离$$ d(c_{j,i}，c_{j,i^{'}}) $$预先计算好，计算时可以通过直接查表。

在KNN检索时，我们需要计算x与数据库中所有y的距离，相比暴力遍历的方法（复杂度为O(nD)），SDC只需要对x做一次量化编码，由于使用查表，遍历时复杂降为O(nm)。

#### 2.2.2.2 ADC

ADC是一种非对称的计算方法，即y仍然使用量化类中心q(y)表示，直接计算x与q(y)的距离

$$ d^{\star}(x, y) = d(x, q(y)) = \sqrt{\sum_{j}^m{d(u_{j}(x), q_{j}(u_{j}(y)))^2}} $$

一般会先将$$u_j(x)$$与$$c_{j,i}$$的距离计算出来，然后在遍历时通过查表计算距离，遍历复杂度为O(nm)。

### 2.2.3 Product Quantization的改进：IVFADC算法

当n很大，比如千万级别，虽然相比暴力搜索算法，PQ算法已经减少了计算量，但计算量O(nm)依旧很大，并不实用。

IVFADC算法，一种基于倒排索引的ADC算法。该算法包含2层量化，第1层被称为粗粒度量化器，在原始的向量空间中，基于Kmeans聚类出k′个簇，第2层是上文讲的PQ量化器，不过这个PQ量化器不是直接在原始数据上做，而是经过第1层量化后，计算出每个数据与其量化中心的残差后，对这个残差数据集进行PQ量化。用PQ处理残差，而不是原始数据的原因是残差的方差更小。

#### 2.2.3.1 建立索引结构

建立索引结构又叫离线建库，建立过程如下图所示：

<img src="/images/large_scale_knn/2.png" width="60%" height="60%">

1. 对于所有的离线数据y，进行一次粗聚类，同时得到每个数据y所对应的粗量化中心$$ q_c(y) $$。  
2. 计算每个y与对应的粗量化类中心$$ q_c(y) $$的残差，$$ r(y)=y-q_{c}(y) $$
3. 使用PQ算法，对r(y)的子向量分别进行聚类，
得到子向量的量化类中心$$ c_{j,i}, 1\leq j \leq m, 1\leq i \leq k $$，进而得到r(y)的PQ量化编码。
4. 对于每个y，在粗量化中心$$ q_c(y) $$对应的倒排列表中插入二元组<y的id，对应的PQ量化编码>

#### 2.2.3.2 检索

<img src="/images/large_scale_knn/3.png" width="60%" height="60%">

1. 对query x进行粗量化，找到最近的w个粗量化类中心，分别与x计算残差
2. 对于第1步中每个粗量化类中心，计算$$u_j(x)$$与相应PQ量化器子向量类中心的距离$$ d(u_{j}(r(x)), c_{j,i})^2 $$
3. 对于第1步中每个粗量化类中心，在相应倒排索引中遍历候选数据y，并查表计算r(x)与y的距离。
4. 选出top-k

### 2.2.4 Annoy搜索算法

[https://segmentfault.com/a/1190000013713357](https://segmentfault.com/a/1190000013713357)

##  参考资料

[http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017/08/05/understanding-product-quantization](http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017/08/05/understanding-product-quantization)

[http://willzhang4a58.github.io/2016/05/sphd/](http://willzhang4a58.github.io/2016/05/sphd/)

[https://segmentfault.com/a/1190000013713357](https://segmentfault.com/a/1190000013713357)
